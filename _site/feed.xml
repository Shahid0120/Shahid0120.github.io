<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-01T13:27:05+10:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Shahid Hussain | Blog</title><subtitle>A Data Science and Mathematics blog, focusing on Machine learning algorithms.</subtitle><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><entry><title type="html">Data Viz Recap - The Good, the Bad and the Biased: Five ways Visaulisations can mislead (and how to fix them) by Danielle Szafir</title><link href="http://localhost:4000/data%20visualisation/Data-viz-review-the-good-the-bad-the-biased/" rel="alternate" type="text/html" title="Data Viz Recap - The Good, the Bad and the Biased: Five ways Visaulisations can mislead (and how to fix them) by Danielle Szafir" /><published>2024-12-08T00:00:00+11:00</published><updated>2024-12-08T00:00:00+11:00</updated><id>http://localhost:4000/data%20visualisation/Data-viz-review-the-good-the-bad-the-biased</id><content type="html" xml:base="http://localhost:4000/data%20visualisation/Data-viz-review-the-good-the-bad-the-biased/">&lt;p&gt;Today I read a paper by Danielle Albers Szafir from University of Colorado Boulde, where she explores thew ways in which visualisation can mislead audiences.&lt;br /&gt;
I wanted to give a breif summary of what I’ve found!&lt;/p&gt;

&lt;h2 id=&quot;when-why-how&quot;&gt;When, why, how?&lt;/h2&gt;

&lt;p&gt;Visualisation shows a unidersaility visual stratucvutiorn when statistics fall short! Szafir list a criteria for assessing data visualisation which is a trade-off of flexibility vs presision. These include;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Uncertainty (does staticis represent data?)&lt;/li&gt;
  &lt;li&gt;transparency (What does the underlying data look like?)&lt;/li&gt;
  &lt;li&gt;context (does visulisation aid in out decision making process?)&lt;/li&gt;
  &lt;li&gt;scale (how many diustinc quantities do we need to evaluate?)&lt;/li&gt;
  &lt;li&gt;exposition (what story does our data need to tell?)&lt;/li&gt;
  &lt;li&gt;purpose (do we know what we are looking for?)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-viz-recap-the-good-the-bad-the-bais/fig-1.png&quot; alt=&quot;Screenshot 2024-12-08 at 10.16.51 am.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;steps-in-visualising-data-which-step-dooes-misleading-visualisation-occur&quot;&gt;Steps in Visualising Data. Which step dooes misleading visualisation occur?&lt;/h2&gt;

&lt;p&gt;Steps to visualisating raw data include;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Clean Data&lt;/li&gt;
  &lt;li&gt;Precompute relevant information, map that information to different visual channels (e.g., position, size, color)&lt;/li&gt;
  &lt;li&gt;Integrate interaction and other details where appropriate.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Step 2 is when problems with visualisation occurs!&lt;/p&gt;

&lt;h1 id=&quot;lessons&quot;&gt;Lessons&lt;/h1&gt;

&lt;h2 id=&quot;lesson-1--dont-used-rainbow-colourmaps-for-continous-vairables&quot;&gt;Lesson 1 : Don’t used rainbow colourmaps for continous vairables&lt;/h2&gt;

&lt;p&gt;Visualising using the rainbow coulour hue can be lead to misleading&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Visual intepretation: People see pattern in colour, thus using raindbow shgow colour changes misleading people about data interpretation&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bias colour blind people : 1 in 12 people care colour blind, thus interpret rainbow colours didferently!&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/data-viz-recap-the-good-the-bad-the-bais/fig-2.png&quot; alt=&quot;Screenshot 2024-12-08 at 10.26.06 am.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;lesson-2-dont-use-animation-as-people-can-see-subtle-changes-year-to-year&quot;&gt;Lesson 2: Don’t use animation as people can see subtle changes year to year!&lt;/h2&gt;

&lt;p&gt;Human suffer from ‘change blindness’ - people lose sight of most data points over multiple graphs! Thus without guidence is can be hard for people to intepret change. To prevent these there are 3 main types with different prupose to show change;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Jaxposition - used in large datapoint but hard to specifically show difference&lt;/li&gt;
  &lt;li&gt;Superposition - show immediate changes, but limited to only couple data points&lt;/li&gt;
  &lt;li&gt;Explicty encoding - Show indvidual difference, but analyss need to intepret which is most important!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-viz-recap-the-good-the-bad-the-bais/fig-3.png&quot; alt=&quot;Screenshot 2024-12-08 at 10.32.09 am.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lesson-3--scale-are-important&quot;&gt;Lesson 3 : Scale are important!&lt;/h2&gt;

&lt;p&gt;The main problem is impropoer normalisation and comparing different graphs using different scales.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-viz-recap-the-good-the-bad-the-bais/fig-4.jpeg&quot; alt=&quot;Screenshot 2024-12-08 at 10.37.12 am.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case both scales are different y-axis, misleading viewers that abortions has increased to more then cancer screening.&lt;/p&gt;

&lt;p&gt;Improving these scale&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Always start from 0 axis&lt;/li&gt;
  &lt;li&gt;If wanting to show change, then computer changes relative to some baseline&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lesson-4-3d-visualisation-sufffers&quot;&gt;Lesson 4: 3D visualisation sufffer’s&lt;/h2&gt;

&lt;p&gt;3d animations suffer from 3 main flaws;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;occlution - object make other object hard to view&lt;/li&gt;
  &lt;li&gt;projection - our netual visual cues distort 3d images, by where object that a further away looks smaller&lt;/li&gt;
  &lt;li&gt;perceptual ambiguity&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-viz-recap-the-good-the-bad-the-bais/fig-5.png&quot; alt=&quot;Screenshot 2024-12-08 at 10.45.27 am.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case occlusion offuce in the 3D image since we can see the middle bars. Porjection occur where the right corner bar look small then the front bar even they there are the same size, causing perceptual ambiguity.&lt;/p&gt;

&lt;h2 id=&quot;lesson-5-show-dont-tell&quot;&gt;Lesson 5: Show don’t tell&lt;/h2&gt;

&lt;p&gt;When makling decision with algorithms is imrpotant, but making these decision without understanding the strcutuvre of data can lead to misinformed decision making!&lt;/p&gt;

&lt;p&gt;When consdiering viosualisation and staticis found when exmaing the visuyalisation allows used to under the strucutre of data!&lt;/p&gt;

&lt;p&gt;For example when we consdier the mean of 3 different approaches they look the same (left), but the udnerlying distribution of data points (right) present the better options would be approach A! This is known as the ‘within the bar bais’, a bias we add through bar charts!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data-viz-recap-the-good-the-bad-the-bais/fig-6.png&quot; alt=&quot;Screenshot 2024-12-08 at 11.01.45 am.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;My main takeways is there is allot oif misinformation when looking at in charts, and as analyst we must conduct exploratory data analysis to ensure we make the right decisions!&lt;/p&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="Data Visualisation" /><summary type="html">Today I read a paper by Danielle Albers Szafir from University of Colorado Boulde, where she explores thew ways in which visualisation can mislead audiences. I wanted to give a breif summary of what I’ve found!</summary></entry><entry><title type="html">Atlassian x Alliance Datathons! Lesson I’ve Learnt</title><link href="http://localhost:4000/datathons/Atlassian-Allianz-data-soc/" rel="alternate" type="text/html" title="Atlassian x Alliance Datathons! Lesson I’ve Learnt" /><published>2024-08-06T00:00:00+10:00</published><updated>2024-08-06T00:00:00+10:00</updated><id>http://localhost:4000/datathons/Atlassian-Allianz-data-soc</id><content type="html" xml:base="http://localhost:4000/datathons/Atlassian-Allianz-data-soc/">&lt;p&gt;Recently I’ve had the opportunity to to participate compete in the UNSW Atlassian Allianz Data Competition at their respective headquarters! This 3-day event required my team to classify fraudulent customers from insurance data collected by Allianz customers.&lt;/p&gt;

&lt;p&gt;Luckily, we made it to the final round, comprising 4 teams, and ended up coming 3rd out of 75 teams! here’s the &lt;a href=&quot;https://github.com/Shahid0120/atlassian-allianz-data-competition&quot;&gt;Project Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The objective of the task was to classify fraudulent insurance cases based on consumer history, profile, demographics, and geographics. This was my first time competing in an event like this, and although I was a little nervous, it was one of the best experiences I’ve had. Working together in a team to solve data problems, I can’t wait until I get a job in the field! Anyways, I just wanted to share my insights and lessons learned from senior Allianz data scientists on our presentation and what I overall learned from the experience.&lt;/p&gt;

&lt;h1 id=&quot;1-interpretability-is-king&quot;&gt;1. Interpretability is KING&lt;/h1&gt;

&lt;p&gt;“I’ve been recently looking at a lot of Kaggle competition winners and their solutions to problems. Additionally, I recently read a paper on the &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0169207021001874&quot;&gt;M5 forcasting competition&lt;/a&gt;. What I found is that although traditional models like ARIMA were used, the majority used LightGBM and Neural Networks. The problem is that only one solution in the top 20 used classical statistics, so just by reading these Kaggle competitions, I was given false hope about learning these complex models, which are not really interpretable.&lt;/p&gt;

&lt;p&gt;In the industry, you will always have to present your findings to a non-technical manager or employee. Thus, being able to accurately explain your model in depth is required, and you can’t really do it if you’re ensembling multiple models.”&lt;/p&gt;

&lt;h1 id=&quot;2-simplicity-is-always-the-way-to-go-forward&quot;&gt;2. Simplicity is always the way to go forward&lt;/h1&gt;

&lt;p&gt;We focused on using decision trees, including CatBoost and LightGBM, and the other team focused on these models as well. Interestingly, the winner ended up using Lasso regression. Funny enough, I was talking to the group prior to the presentation, and when I heard Lasso regression, I thought, ‘Hmm… that’s more aimed at feature selection.’ But the important thing is that it’s a simple model that you can easily explain to non-technical managers!&lt;/p&gt;

&lt;h1 id=&quot;3-industrial-standards-are-important-dont-just-use-a-methods-without-researching-academic-papers-on-its-effectiveness-and-cons&quot;&gt;3. Industrial standards are important, don’t just use a methods without researching academic papers on its effectiveness and con’s!&lt;/h1&gt;

&lt;p&gt;The label set was unbalanced, representing more non-fraudulent cases than fraudulent cases, which makes sense. Of course, we had to address this issue; if we didn’t, we wouldn’t get meaningful results. From all the blog posts and general discussions, the most popular method is SMOTE-N, so we thought, ‘Great, easy,’ and we did it.&lt;/p&gt;

&lt;p&gt;During our presentation, a key criticism was that ‘SMOTE doesn’t really work in reality; it’s not used in the industry since you are making artificial data points. You should have used Weighted Oversampling.’ Now, of course, if I or the team had read about the industrial performance of oversampling methods, we could have identified this!&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This was an invaluable experience. I can’t express my gratitude enough for the opportunity to participate in such an event. If one day I get the opportunity to be a judge at this event when I’m a senior data scientist, I will welcome it with open arms.&lt;/p&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="Datathons" /><summary type="html">Recently I’ve had the opportunity to to participate compete in the UNSW Atlassian Allianz Data Competition at their respective headquarters! This 3-day event required my team to classify fraudulent customers from insurance data collected by Allianz customers.</summary></entry><entry><title type="html">Episode Two (IBM x UNSW machine learning project) : Learning Models</title><link href="http://localhost:4000/ml/episdoe-two-datasoc-ibm/" rel="alternate" type="text/html" title="Episode Two (IBM x UNSW machine learning project) : Learning Models" /><published>2024-05-31T00:00:00+10:00</published><updated>2024-05-31T00:00:00+10:00</updated><id>http://localhost:4000/ml/episdoe-two-datasoc-ibm</id><content type="html" xml:base="http://localhost:4000/ml/episdoe-two-datasoc-ibm/"></content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="ML" /><summary type="html"></summary></entry><entry><title type="html">Episode One (IBM x UNSW machine learning project) : Data Preprocessing</title><link href="http://localhost:4000/ml/episode-one-data-scoc-ibm/" rel="alternate" type="text/html" title="Episode One (IBM x UNSW machine learning project) : Data Preprocessing" /><published>2024-05-31T00:00:00+10:00</published><updated>2024-05-31T00:00:00+10:00</updated><id>http://localhost:4000/ml/episode-one-data-scoc-ibm</id><content type="html" xml:base="http://localhost:4000/ml/episode-one-data-scoc-ibm/">&lt;p&gt;Today was the first day of attending a term-long project with Data Society UNSW, in collaboration with IBM. The lecturer from Dr. Saeed Kasmani, a Senior AI engineer from IBM presented the fundamentals of data science workflow. The main premise is that throughout the term, we will try to optimize a machine learning algorithm with a dataset. During the term, an IBM machine learning engineer will teach us the fundamentals of machine learning, including data preprocessing and learning algorithms.&lt;/p&gt;

&lt;h1 id=&quot;the-client-and-objective&quot;&gt;The client and objective&lt;/h1&gt;

&lt;p&gt;Your client is a multinational real estate developer that builds residential and commercial properties 
around the world. They have a large portfolio of projects that are in development simultaneously. They 
currently have a 25% failure rate for their projects that is significantly higher than the industry 
benchmark of less than 10%. They would like to understand what the key leading indicators for project 
failure are when they are planning their projects. This will allow them to only invest capital into the best 
quality projects. They also want to know ongoing which projects are likely to fail so they can cut their 
losses and cease the projects. Secondly, the real estate developer would like to search local building 
policies to quickly find the relevant answers as they plan their developments. This could include zoning 
policies, environmental policies and development planning policies&lt;/p&gt;

&lt;h1 id=&quot;break-down-the-client-objective&quot;&gt;Break down the client objective&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Improve picking projects - find the likelihood of project failure&lt;/li&gt;
  &lt;li&gt;Reduced speed to policy search&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;weekly-project-overview&quot;&gt;Weekly project Overview&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ibm/ibm-weekly-break.png&quot; alt=&quot;weeklyoverview&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;data-preprocessing&quot;&gt;Data preprocessing&lt;/h2&gt;

&lt;p&gt;Our main objective before making any inference of learning models is to understand the fundamental question&lt;/p&gt;

&lt;h3 id=&quot;does-our-data-useful-proxy-for-understanding-our-problem&quot;&gt;Does our data useful ‘proxy’ for understanding our problem?&lt;/h3&gt;

&lt;p&gt;This is known as exploratory data analysis, for which is a process done after the data is usable.&lt;/p&gt;

&lt;h2 id=&quot;what-is-usable-data&quot;&gt;What is usable data?&lt;/h2&gt;
&lt;p&gt;Making a usable dataset is the process&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Data Wrangling - sourcing, loading, and precleaning the 
data so we can see what it really looks like, fixing critical issues&lt;/li&gt;
  &lt;li&gt;Data profiling and cleaning - understanding the essential characteristics of the data , applying preliminary transformations to  confer context and meaning. Continuing implementing strategies for missing and invalid data (Imputations methods)&lt;/li&gt;
  &lt;li&gt;Data mugging - reshaping data to prepare it for analysis&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One important lesson is that this is not a linear cycle, from Data Wrangling to Data mugging to learning models. It a iterative process for which we will continuously revise our data to import our analysis and learning model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ibm/IBM_data_cleaning_fixing.jpg&quot; alt=&quot;data-cleaning&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;how-to-go-about-data-cleaning-and-profiling&quot;&gt;How to go about Data Cleaning and Profiling?&lt;/h1&gt;

&lt;p&gt;After conducting basic data wrangling, you would proceed to preliminary data cleaning, including reformatting, data type conversion, and dealing with dirty data. Secondly, basic data profiling includes finding data types, data ranges (continuous), and categories. Thirdly, one of the most important parts of this stage is ‘assessing data quality’.&lt;/p&gt;

&lt;p&gt;When assessing data quality, we look at accuracy, reliability (veracity), currency, and relevance. This covers how many invalid or missing values each feature, row, or the overall dataset has, and what the effect is of throwing out or imputing the data.&lt;/p&gt;

&lt;p&gt;Remember, in learning algorithms, we assume each sample (i.e., each row) is part of some unknown distribution. That is to say, even the samples that are missing play a part. Therefore, by removing or imputing samples, we risk changing the distribution of the training set, which can result in large test errors!&lt;/p&gt;

&lt;h1 id=&quot;visualizing-data&quot;&gt;Visualizing Data&lt;/h1&gt;

&lt;p&gt;After this process is done, we are now ready to explore our data visually. Through this process, we can identify skewness, the mean, and even outliers. Graphs include scatterplots, wireframe plots, surface plots, histograms, box-whisker plots, and heat maps. After all these processes, we finally begin to understand the data and the features of each column.&lt;/p&gt;

&lt;h1 id=&quot;final-steps-before-trying-a-learning-model&quot;&gt;Final steps before trying a learning model&lt;/h1&gt;

&lt;p&gt;Yay, we are on our last step before the big one: ML algorithms! So now all data is cleaned, we have a solid grasp of the data, and we need to select which data helps us the most in categorizing/predicting our label. This involves three steps: feature engineering, feature reduction, and feature selection!&lt;/p&gt;

&lt;p&gt;Feature engineering in machine learning involves extracting useful features from the given input data, considering the target to be learned and the machine learning model used. It involves transforming data into forms that better relate to the underlying target to be learned. This includes variable transformations such as square root and Box-Cox transformations, and categorical encoding, like one-hot encoding in sk-learn.&lt;/p&gt;

&lt;p&gt;Feature reduction and selection involve reducing the number of variables. This is more commonly done under unsupervised learning using techniques such as PCA, factor analysis, and t-SNE. Amazingly, sk-learn has a whole user guide on feature selection!&lt;/p&gt;

&lt;p&gt;And there we have it! Now, we know the basic outline of data preprocessing. Just a little note: finding valuable, straight-to-the-point information on data preprocessing is extremely difficult. Some textbooks cover the basics, while others go too in-depth. There isn’t anything that’s just enough to perform a basic data science project.&lt;/p&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="ML" /><summary type="html">Today was the first day of attending a term-long project with Data Society UNSW, in collaboration with IBM. The lecturer from Dr. Saeed Kasmani, a Senior AI engineer from IBM presented the fundamentals of data science workflow. The main premise is that throughout the term, we will try to optimize a machine learning algorithm with a dataset. During the term, an IBM machine learning engineer will teach us the fundamentals of machine learning, including data preprocessing and learning algorithms.</summary></entry><entry><title type="html">Understanding the Roles/Links between Samples, Inference and Learning Models</title><link href="http://localhost:4000/ml/sampling-inference-learning/" rel="alternate" type="text/html" title="Understanding the Roles/Links between Samples, Inference and Learning Models" /><published>2024-05-29T00:00:00+10:00</published><updated>2024-05-29T00:00:00+10:00</updated><id>http://localhost:4000/ml/sampling-inference-learning</id><content type="html" xml:base="http://localhost:4000/ml/sampling-inference-learning/">&lt;p&gt;I’v alway been confused of the role of statistical inference, sampling and learning models and how they all connect together. Although, I know statistical inference, it was unfortunately taught in a black box, so i always felt little incomplete when i begin learning Machine learning and modern statistical inference. How do these two connect? Well hopefully i can bridge this gap!&lt;/p&gt;

&lt;h1 id=&quot;sampling-who-are-you&quot;&gt;Sampling? Who are you&lt;/h1&gt;
&lt;p&gt;When learning about phenomenon’s we want to study we ask everyone in the best method would be to ask population right, so the population;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Set of all elements from who the sample will be drawn
Unfortunately, this is not always possible because of financial resource issues. But in some cases it is done, when?
Umm election? Everyone in the population has to vote on who there president is, but the question to ask is how does this news corporations show polls of potential presidential ranking before the election is finished? Well they take the proportion of the population a.k.a sample and hope this sample is a reflection of the whole population!&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;what-are-different-ways-these-news-corporation-could-sample&quot;&gt;What are different ways these news corporation could sample?&lt;/h1&gt;
&lt;p&gt;Generally when taking sample there are two main issues;
(1) Sample Bias - based on the individuals in our sample we can bias a certain outcome which can include selection bias, response bias, nonresponsive bias
(2) Chance error - random samples can vary from what is expected in any direction&lt;/p&gt;

&lt;p&gt;General steps would be creating a sample is;&lt;/p&gt;

&lt;h3 id=&quot;1-identify-our-population&quot;&gt;1. Identify our population&lt;/h3&gt;
&lt;h3 id=&quot;2create-a-sampling-frame&quot;&gt;2.Create a sampling frame&lt;/h3&gt;
&lt;p&gt;a subset of population of potential people who can be in our sample&lt;/p&gt;
&lt;h3 id=&quot;3chose-a-sampling-scheme&quot;&gt;3.Chose a sampling scheme&lt;/h3&gt;
&lt;p&gt;note sampling method quality is more important then sampling size a few sampling methods includes;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Convenience sample: is whoever is available we take them as our sample, but this has high bias&lt;/li&gt;
  &lt;li&gt;Deterministic Sampling: When we specifically choose from our sampling frame, no chance&lt;/li&gt;
  &lt;li&gt;probability/random sample : Given a sampling frame we give ‘chance’ of anyone being selected in our sample, thus there is a randomness about this. Through the use of probability we can know measure errors and thus we have reduced bias by not sampling non-randomly. The most common random sampling methods are
    &lt;ul&gt;
      &lt;li&gt;Systematic sample : randomly align all potential individuals in a line, then random choose from the start of sequences and then evenly choose between each individual to create the sample&lt;/li&gt;
      &lt;li&gt;uniform random sample with replacement :  uniformly at random with replacement, but the problem is some people may be picked more then once!&lt;/li&gt;
      &lt;li&gt;simple random sample : sample drawn uniformly at random without replacement.&lt;/li&gt;
      &lt;li&gt;stratified random sample : where random sampling are performed on specific group, which each group is combined into a sample , thus stop the unbalanced class!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;given-we-have-our-sample-how-does-new-corporation-make-predictions-about-population&quot;&gt;Given we have our sample how does new corporation make predictions about population&lt;/h1&gt;
&lt;p&gt;A ‘statistic’ , a number calculated from the sample and a ‘parameter’, a number associated with the population, now a statistic can be calculated from our sample can be used as an estimate of our ‘parameter’ which we cant feasible measure . 
Think Sample Mean vs Population Mean, \(\overline{X_n}\) and \(\mu\). Here Sample Mean is our statistic which estimates the population mean.&lt;/p&gt;

&lt;p&gt;Now defining statistical inference; is using our ‘random sample’ we make conclusion about the population, i.e our we hope our sample mean is close to the population mean which we don’t know. How can we check our sample statistic/estimator is close to our population paramter?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Variance: How does my statistic changes with each sample: \(Var(\theta) = \mathbb{E}((\hat{\theta} - \theta)^2)\)&lt;/li&gt;
  &lt;li&gt;Bias : Did we get the right answer to our parameter? \(Bias(\hat{\theta})= \mathbb{E}(\hat{\theta} - \theta)\)&lt;/li&gt;
  &lt;li&gt;Risk/MSE = Variance + Bias^2 : How close is our parameter ? \(MSE(\hat{\theta}) = \mathbb{E}(\hat{\theta} - \theta)^2\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So for our new coporation they will want to find the sample mean for votes for say trump.&lt;/p&gt;

&lt;h1 id=&quot;learning-models&quot;&gt;Learning Models&lt;/h1&gt;
&lt;p&gt;Now, when building models we have either two goals, 
(1) Prediction of unseen data, regression or classification
(2) Inference: draw conclusions about the underlying relationships between features and response, so training is the process of fitting a model while inference is making the prediction&lt;/p&gt;

&lt;p&gt;Now lets take an example, suppose  we have a simple linear regression, our population parameter is which we want to find but we don’t know,&lt;/p&gt;

\[\begin{align*}
y = \beta_0 + \beta_1 x_0  
\end{align*}\]

&lt;p&gt;Now using our random sample we get our estimated parameters,&lt;/p&gt;

\[\begin{align*}
y &amp;amp;= b_0 + b_1 x_0  \Longleftrightarrow  \\
y &amp;amp;= 9.3 + 4.3 x_0
\end{align*}\]

&lt;p&gt;Okay so our estimated statistic/estimator \(b_1 = 4.3\) so then can we conclude that our parameter is \(\beta_1 = 4.3\) ? Is this right? No! Well not right now we only considered 1 random sample, we could have totally got it wrong, since we hadn’t compared it to any other samples!&lt;/p&gt;

&lt;p&gt;So to find the true parameter \(\beta_1\) we make conclusion from the distribution/probability of statistic \(b_1\) from all possible random samples! And then we use hypothesis testing check check if potentially is \(\beta_1 = 4.3\).&lt;/p&gt;

&lt;h1 id=&quot;how-can-we-do-this-hypothesis-testing&quot;&gt;How can we do this hypothesis testing?&lt;/h1&gt;

&lt;p&gt;Outlined in “Computation Age of Statistical Inference” by Bradley Efron and Trevor Hastie, we can check our estimator from classifical statistical inference methods like Taylor-series approximations, plug-in principle to modern methods including bootstrapping, Jackknife resampling.&lt;/p&gt;

&lt;p&gt;Let’s take Jackknife resampling. \&lt;/p&gt;

&lt;p&gt;This is a resampling method invented by Quenouille in 1949 for the sole purpose of reducing bias of estimators! In this method since it is always difficult to resample from the population, so jackknife resampling takes N-subsample, in each subsample 1 individual taken out each time so the subsample size is always N - 1, since we have N individuals in our original sample, thus the every individual in the original sample is taken out atleast once. Then we do our simple linear regression again and get our \(b_1\) statistic, a estimate of our parameter \(\beta_1\), we then taken the mean of all \(b_1\) from each sample&lt;/p&gt;

\[\begin{align*}
\overline{b_1} = \frac{1}{n - 1} \sum_{i = 1}^n b_{1,i}
\end{align*}\]

&lt;p&gt;Therefore, we know from our subsample some \(b_1\) will over shoot our original parameter and some subsample \(b_1\) will undershoot, but we hope on average \(\overline{b_1} = \beta_1\) .&lt;/p&gt;

&lt;p&gt;Since, we have a range of \(b_1\) values we can create a empirical probability density distribution graph and then find 95% confidence interval \(b_1\)
So our hypothesis is
\(\begin{align*}
H_0 &amp;amp;: b_1 = 4.3 \\ 
H_a &amp;amp;: b_1 \neq 4.3
\end{align*}\)&lt;/p&gt;

&lt;p&gt;So we conduct jackknife sampling to find the jackknife empirical distribution of statistic \(b_1\);&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/inference-learning-sampling/Screenshot%202024-06-30%20at%2012.17.55%20pm.png&quot; alt=&quot;sampling-95-inference&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly,&lt;/p&gt;

\[CI(b_0) = [2.92, 5.62] = [2.5\%, 97.5\%]\]

&lt;p&gt;so we don’t have enough evidence to reject our null hypothesis. Thus our regression model is valid&lt;/p&gt;

\[\begin{align*}
y &amp;amp;= 9.3 + 4.3 x_0
\end{align*}\]

&lt;p&gt;Note : if our model doesn’t hold say, the relationship between y and x is not linear then the inference also doesn’t hold!&lt;/p&gt;

&lt;p&gt;So now we have a learning model which holds true and thus we can predict future values of \(y\) using \(x_0\)&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Now both you and me both understand the relationship between sampling, inference and learning models!&lt;/p&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="ML" /><summary type="html">I’v alway been confused of the role of statistical inference, sampling and learning models and how they all connect together. Although, I know statistical inference, it was unfortunately taught in a black box, so i always felt little incomplete when i begin learning Machine learning and modern statistical inference. How do these two connect? Well hopefully i can bridge this gap!</summary></entry><entry><title type="html">Imputation : Methods</title><link href="http://localhost:4000/ml/Imputation/" rel="alternate" type="text/html" title="Imputation : Methods" /><published>2024-05-28T00:00:00+10:00</published><updated>2024-05-28T00:00:00+10:00</updated><id>http://localhost:4000/ml/Imputation</id><content type="html" xml:base="http://localhost:4000/ml/Imputation/">&lt;p&gt;Today for my IBM project I wanted to learn more about imputation methods, selection of features, types. The following is from &lt;a href=&quot;https://stefvanbuuren.name/fimd/&quot;&gt;Flexible Imputation of Missing Data&lt;/a&gt; by Stef Van Buuren .&lt;/p&gt;

&lt;h1 id=&quot;types-of-missing-data&quot;&gt;Types of Missing data&lt;/h1&gt;
&lt;p&gt;Let $D = {(x,y)^i}$  and $X$ be our missing data column where $X := (X_{obs}, X_{missing})$ for $\theta$ being the parameter of missing values then;&lt;/p&gt;

&lt;p&gt;(1)Missing completely at random (MCAR) - Unlucky out of control missing values that is 
\(Pr(R = 0 |X_{obs}, X_{missing}, \theta) = Pr(R=0| \theta)\) .&lt;/p&gt;

&lt;p&gt;(2)Missing at random (MAR) - through our asusmption of sampling, data is missing that is 
\(Pr(R = 0 |X_{obs}, X_{missing}, \theta) = Pr(R=0| X_{obs}, \theta)\) .&lt;/p&gt;

&lt;p&gt;(3)Not missing at random (NMAR): 
\(Pr(R = 0 |X_{obs}, X_{missing}, \theta)\).&lt;/p&gt;

&lt;h1 id=&quot;commonly-used-imputation-are-problematic&quot;&gt;Commonly used Imputation are Problematic&lt;/h1&gt;
&lt;h2 id=&quot;method-1--complete-case-anlysis---deleting-samples-why-any-features-missing&quot;&gt;Method 1 : (Complete Case Anlysis) - deleting samples why any features missing&lt;/h2&gt;
&lt;p&gt;Revmoing all samples where there is a missing input. that is if i have D = (200, 30) if one of the index = 189 has one missing values in column = 5 then i would remove the whole index, but even thoruhg removing missing values is fast this assume missing data is MCAR,. If this assumption is not true then by removing these missing values is casue estimators for future paramettric models, such as linear regression etc to have bias estimators of (1) mean, (2) regression coefficients (3) correlation.&lt;/p&gt;

&lt;h2 id=&quot;method-2-avaliable-case-anlalysis---not-deleting-any-missing-entries&quot;&gt;Method 2: Avaliable Case Anlalysis - not deleting any missing entries&lt;/h2&gt;

&lt;p&gt;If we are anylysing two features $X_1$ and $X_2$ if both features have different index where data is missing, we don’t remove them missing values. Problems with this method is it assume missing values are MCAR, thus if the assumption is wrong then it bias estimators of future learning models. Secondly, sample size could be different for each category being analysed, leading to complication of analysis of whole dataset.&lt;/p&gt;

&lt;h2 id=&quot;method-3-mean-imputation&quot;&gt;Method 3: Mean Imputation&lt;/h2&gt;

&lt;p&gt;For each features the mean is taken then is placed into each missing values of that coloumn. The problem with this reguards of the type of missing data, this distors the variance. Also if tehre is a positive correlation between vairables then it icnrease the correlation by using eman imputation&lt;/p&gt;

&lt;h2 id=&quot;method-4-regression-imputation&quot;&gt;Method 4: Regression Imputation&lt;/h2&gt;

&lt;p&gt;Viaselection some slection method, we can compute univairate or multivairate regression and then use this information to fill in the missing values. Importantly we are not interpolation each data points, but rather estimated via dataset. The benefits of this method is if the data is MCAR or MAR has unbias estimatros for future parametirc learning models, but by using the relationship between variables and inputing these values into missing data point, we icnrease the correlation between the used features vairables in the regression model. Addtionally, it decrease the samples variables in sample, thus cause underestimation of variatability.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/imputation/regression-imputation.png&quot; alt=&quot;regression-imputation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even though this is the most commonly used method for imputation is it extremelt dagerous due to increased correlaotion (relation) between vairables, which cause more false positives in future learning models.&lt;/p&gt;

&lt;h2 id=&quot;method-5-stochastic-regression-imputation-sri&quot;&gt;Method 5: Stochastic regression imputation (SRI)&lt;/h2&gt;
&lt;p&gt;Doing Regression Imputation, but now we add noise to decrease correlation bias.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/imputation/stochastic-regression-imputation.png&quot; alt=&quot;sotchastic-regression-imputation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even thought the benfits of this model is it does not underestimate the variance, it has larger sample variacne of estimators then compared to other imptuation. Addtionaly, the standard error of learning models using on features using SRI n have are very small, thus dont adress uncertainty of estimation! Continuing, SRI genrally doesnt adress higher end of frequency, can randomly assign noise negative values even thought it might not be plausible (weight can be negative)&lt;/p&gt;

&lt;h2 id=&quot;summary-of-commonly-used-imputation-methods&quot;&gt;Summary of Commonly used Imputation methods&lt;/h2&gt;
&lt;p&gt;All these methods have inherent problems, effecting bias of future learning model estimators or not identifying the correct distribution. All the methods above are known as single imputation, that it they only given a method mean imputation, SRI imputation they all predict a single data value for each missing data point. Whihc increases biais since how do we know that our current sample is a accurate representation of the whole population/distribution?&lt;/p&gt;

&lt;h1 id=&quot;current-solutions-to-imputations--multiple-imputation&quot;&gt;Current Solutions to imputations : Multiple Imputation&lt;/h1&gt;

&lt;h2 id=&quot;multiple-imputation&quot;&gt;Multiple Imputation&lt;/h2&gt;
&lt;p&gt;Generally when we conduct supervised learning we can do it with one feature (X, Y) or with multiples features to predict one label.&lt;/p&gt;

&lt;h3 id=&quot;univariate-multiple-imputation&quot;&gt;&lt;em&gt;Univariate Multiple Imputation&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;this is the case when we only have missing values in one feature, there are two types&lt;/p&gt;

&lt;h2 id=&quot;univariate-multiple-imputation--continuous-feature&quot;&gt;Univariate Multiple Imputation : Continuous Feature&lt;/h2&gt;

&lt;p&gt;An example, suppoer we have a dataset, where the columns are weight, eye_colour, nationality, age, income, Body Mass Index, height. Suppose we want to want to make a classification algorithm to find the height of someone. So the potential features are weight, eye_colour, nationality, age, income, BMI. From a basic data profiling we find that age has a missing value in index 10. So what we can do is we find the correlation between (weight and age) have a high correlation, thus we use so what we do first is we split our dataset into say 10 parts. Then we conduct any learning model like regression, linear or non linear. Then from our subsamples we predic the missing values of age. Thus we have 10 different values for age in index 10, then we can one method is to compute the mean of all these estimates for age at index 10 and thus we have now reduced our bias, but using in a sense multiple samples from the unknown distribution, rather then computing it from one sample. The figure brlow illustrates this idea,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/imputation/workflow-missing-multiple-imputaiton.png&quot; alt=&quot;workflow-missing-multiple-imputation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 4 main types of models to estimate the missing index via subsamples (not the missing column) is normally distributed&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Regression Imputation - $$ \hat{y} = \hat{\beta_{0}} + X_{missing}\hat{\beta_{1}}&lt;/li&gt;
  &lt;li&gt;Stochastic Imputation \(\hat{y} = \hat{\beta_{0}} + X_{missing}\hat{\beta_{1}} + \epsilon\) where \(\epsilon ~ N(0, \sigma^2)\)&lt;/li&gt;
  &lt;li&gt;Bayesian multiple imputation - \(\hat{y} = \hat{\beta_{0}} + X_{missing}\hat{\beta_{1}} + \epsilon\), but now rather the above assume frequentist statistics, we use Bayesian Statistics and assume our parameters, \(\hat{\beta_{0}}, \hat{\beta_{1}} \text{ and } \sigma\) are from some posterior distribution we have chosen.&lt;/li&gt;
  &lt;li&gt;Bootstrap Multiple Imputation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note when our \(\hat{y}\) is not normally distributed then we conduct a transformation to make it normally distributed.&lt;/p&gt;

&lt;h2 id=&quot;univariate-multiple-imputation--categorical-feature&quot;&gt;Univariate Multiple Imputation : Categorical Feature&lt;/h2&gt;
&lt;p&gt;In this case we use generalized linear models, specifically logistic regression, for binary classes&lt;/p&gt;

\[\begin{align*}
Pr(y_i | X_i, \beta) = \frac{\exp(X_iB)}{1 + \exp{X_i\beta}}
\end{align*}\]

&lt;p&gt;Multi-class using, multinomial logit model for nominal categorical variables&lt;/p&gt;

\[\begin{align*}
Pr(y_i = K | X_i, \beta) = \frac{\exp(X_iB)}{1 + \exp(X_i\beta)}
\end{align*}\]

&lt;p&gt;Ordered Logit Model for ordinal categorical variables&lt;/p&gt;

&lt;p&gt;\(\begin{align*}
Pr(y_i \leq K | X_i, \beta, \tau_k) = \frac{\exp( \tau_k - X_iB)}{1 + \exp( \tau_k - X_i\beta)}
\end{align*}\)
so for a given category&lt;/p&gt;

\[\begin{align*}
Pr(y_i = k | X_i) = Pr(y_i \leq k | X_i) - Pr(y_i \leq k - 1 | X_i)
\end{align*}\]

&lt;h3 id=&quot;multiple-multiple-imputation&quot;&gt;&lt;em&gt;Multiple Multiple Imputation&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;THre problem wit the above example is that in the real world missing data span different index’es and multiple features, so&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Imputation stronger if not multiples variables are missing for each data&lt;/li&gt;
  &lt;li&gt;Measuring Usability of dataset when multiple missing coloumns and index
    &lt;ul&gt;
      &lt;li&gt;Proportion of usable cases&lt;/li&gt;
      &lt;li&gt;Outbound Statistic, Inbound statistics (for two features measure )&lt;/li&gt;
      &lt;li&gt;Influx and outflux coefficient (comparison of Outbound Statistic, Inbound statistics for multiple featrues )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Issues
    &lt;ul&gt;
      &lt;li&gt;predictors contain missing values&lt;/li&gt;
      &lt;li&gt;Variables are often of different types&lt;/li&gt;
      &lt;li&gt;collinearity can occur&lt;/li&gt;
      &lt;li&gt;he ordering of the rows and columns can be meaningful, e.g., as in longitudinal data&lt;/li&gt;
      &lt;li&gt;the relationshiup between variables can be non-linear&lt;/li&gt;
      &lt;li&gt;Imputation can create impossible combinations, such as pregnant fathers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Methods
    &lt;ul&gt;
      &lt;li&gt;Monotone data imputation. (MICE) For monotone missing data patterns, imputations are created by a sequence of univariate methods;&lt;/li&gt;
      &lt;li&gt;Joint modeling. For general patterns, imputations are drawn from a multivariate model fitted to the data;&lt;/li&gt;
      &lt;li&gt;Fully conditional specification, also known as chained equations and sequential regressions. For general patterns, a multivariate model is implicitly specified by a set of conditional univariate models. Imputations are created by drawing from iterated conditional models.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Analysing Imputation
    &lt;ul&gt;
      &lt;li&gt;Workflows&lt;/li&gt;
      &lt;li&gt;dont use for pool part :
        &lt;ul&gt;
          &lt;li&gt;Average Mean -&amp;gt;   incorrect standard errors, confidence intervals and  p-values -&amp;gt; wrong statistical testing or uncertainty analysis&lt;/li&gt;
          &lt;li&gt;Stack imputed data -&amp;gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Use:
        &lt;ul&gt;
          &lt;li&gt;Repeated analyses&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Identify the best features for imputation: problem when using stepwise model selection:
        &lt;ul&gt;
          &lt;li&gt;When seperated data in subsamples then, The first step involves performing stepwise model selection separately on each imputed dataset, followed by the construction of a new supermodel that contains all variables that were present in at least half of the initial models.&lt;/li&gt;
          &lt;li&gt;Majority. A method that selects variables in the final that appear in at least half of the models.&lt;/li&gt;
          &lt;li&gt;tack. Stack the imputed datasets into a single dataset, assign a fixed weight to each record and apply the usual variable selection methods.&lt;/li&gt;
          &lt;li&gt;Wald. Stepwise model selection is based on the Wald statistic calculated from the multiply imputed data.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Application&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Chosing which imputation model (regresion type etc after subsamples) is hardest choice ou goals is to
    &lt;ul&gt;
      &lt;li&gt;account for the process that created the missing data,&lt;/li&gt;
      &lt;li&gt;preserve the relations in the data, and&lt;/li&gt;
      &lt;li&gt;preserve the uncertainty about these relations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Choise we need to make
    &lt;ul&gt;
      &lt;li&gt;MAR assumption is plausible ?&lt;/li&gt;
      &lt;li&gt;form of the imputation model. The form encompasses both the structural part and the assumed error distribution?&lt;/li&gt;
      &lt;li&gt;set of variables to include as predictors in the imputation model. The general advice is to include as many relevant variables as possible, including their interactions&lt;/li&gt;
      &lt;li&gt;impute variables that are functions of other (incomplete) variables (like my time diff/ratio’s)&lt;/li&gt;
      &lt;li&gt;the order in which variables should be imputed.&lt;/li&gt;
      &lt;li&gt;setup of the starting imputations and the number of iterations&lt;/li&gt;
      &lt;li&gt;The number of multiply imputed datasets&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generally&lt;/li&gt;
  &lt;li&gt;The MAR assumption is often a suitable starting point. If the MAR assumption is suspect for the data at hand, a next step is to find additional data that are strongly predictive of the missingness, and include these into the imputation model. If all possibilities for such data are exhausted and if the assumption is still suspect, perform a concise simulation study as in Collins, Schafer, and Kam (2001) customized for the problem at hand with the goal of finding out how extreme the MNAR mechanism needs to be to influence the parameters of scientific interest. Finally, use a nonignorable imputation model (cf. Section 3.8) to correct the direction of imputations created under MAR. Vary the most critical parameters, and study their influence on the final inferences. Section 9.2 contains an example of how this can be done in practice.&lt;/li&gt;
  &lt;li&gt;In my experience, the increase in explained variance in linear regression is typically negligible after the best, say, 15 variables have been included. For imputation purposes, it is expedient to select a suitable subset of data that contains no more than 15 to 25 variables. provide the following strategy for selecting predictor variables from a large database:
    &lt;ul&gt;
      &lt;li&gt;Include all variables that appear in the complete-data model, i.e., the model that will be applied to the data after imputation, including the outcome (Little 1992; Moons et al. 2006). Failure to do so may bias the complete-data model, especially if the complete-data model contains strong predictive relations. Note that this step is somewhat counter-intuitive, as it may seem that imputation would artificially strengthen the relations of the complete-data model, which would be clearly undesirable. If done properly however, this is not the case. On the contrary, not including the complete-data model variables will tend to bias the results toward zero. Note that interactions of scientific interest also need to be included in the imputation model.&lt;/li&gt;
      &lt;li&gt;In addition, include the variables that are related to the nonresponse. Factors that are known to have influenced the occurrence of missing data (stratification, reasons for nonresponse) are to be included on substantive grounds. Other variables of interest are those for which the distributions differ between the response and nonresponse groups. These can be found by inspecting their correlations with the response indicator of the variable to be imputed. If the magnitude of this correlation exceeds a certain level, then the variable should be included.&lt;/li&gt;
      &lt;li&gt;In addition, include variables that explain a considerable amount of variance. Such predictors help reduce the uncertainty of the imputations. They are basically identified by their correlation with the target variable.&lt;/li&gt;
      &lt;li&gt;Remove from the variables selected in steps 2 and 3 those variables that have too many missing values within the subgroup of incomplete cases. A simple indicator is the percentage of observed cases within this subgroup, the percentage of usable cases (cf. Section 4.1)&lt;/li&gt;
      &lt;li&gt;In practice, there is often a small set of key variables, for which imputations are needed, which suggests that steps 1 through 4 are to be performed for key variables only. The mice package contains several tools that aid in automatic predictor selection. The quickpred() function is a quick way to define the predictor matrix using the strategy outlined above. The flux() function was introduced in Section 4.1.3. The mice() function detects multicollinearity, and solves the problem by removing one or more predictors for the model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Say our time_diff = based on end_time and start_time if we impute all these values we will have values for that index which don’t make sense, The easiest way to deal with the problem is to leave any derived data outside the imputation process.&lt;/li&gt;
  &lt;li&gt;Conditional imputation : In some cases it makes sense to restrict the imputations, possibly conditional on other data. The method in Section 1.3.5 produced negative values for the positive-valued variable Ozone. One way of dealing with this mismatch between the imputed and observed values is to censor the values at some specified minimum or maximum value. The mice() function has an argument called post that takes a vector of strings of R commands. These commands are parsed and evaluated just after the univariate imputation function returns, and thus provide a way to post-process the imputed values. Note that post only affects the synthetic values, and leaves the observed data untouched. The squeeze() function in mice replaces values beyond the specified bounds by the minimum and maximal scale values. A hacky way to ensure positive imputations for Ozone under stochastic regression imputation is&lt;/li&gt;
  &lt;li&gt;Interaction terms&lt;/li&gt;
  &lt;li&gt;Quadratic relations
Algorithms type
    &lt;ul&gt;
      &lt;li&gt;Visit sequence&lt;/li&gt;
      &lt;li&gt;Convergence
Diagnostics&lt;/li&gt;
      &lt;li&gt;Model fit versus distributional discrepancy&lt;/li&gt;
      &lt;li&gt;Diagnostic graphs&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="ML" /><summary type="html">Today for my IBM project I wanted to learn more about imputation methods, selection of features, types. The following is from Flexible Imputation of Missing Data by Stef Van Buuren .</summary></entry><entry><title type="html">Evaluating You’re Clustering Model</title><link href="http://localhost:4000/ml/evluating-clustering-model/" rel="alternate" type="text/html" title="Evaluating You’re Clustering Model" /><published>2024-05-25T00:00:00+10:00</published><updated>2024-05-25T00:00:00+10:00</updated><id>http://localhost:4000/ml/evluating-clustering-model</id><content type="html" xml:base="http://localhost:4000/ml/evluating-clustering-model/">&lt;p&gt;This is a continuation of my previous blog post about “Hierarchial Clustering Algorithms”. In this blog i hope to teach you teach you a bit about how to evaluate you’re clustering algorithm.&lt;/p&gt;

&lt;p&gt;Generally clustering is a form of unsupervised learning that is we dont have any labels, thus evaluvating a clustering algoirthm can be a diffult job. Although, clustering can be used to classify samples which form a semi-supervised learning. Alternativly, if the training data uses the labels as a method of optimisation then clustering can be in the form of supervised learning&lt;/p&gt;

&lt;h1 id=&quot;types&quot;&gt;Types&lt;/h1&gt;
&lt;p&gt;Evaluation metric fall under two main catagories :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Internal - when no label for evaluation (unsupervised) we use the density of cluster, and ingerent properiteis of data to evluate the clusters&lt;/li&gt;
  &lt;li&gt;External - givne we have a label for the clusters we can evalute each samples using this label, importantly we didnt use these labels in the traning phase&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;external&quot;&gt;External&lt;/h1&gt;
&lt;p&gt;These clustering eveluation method rely on being able to  have labels. How this is evaluted is done throuhg using Indicator functions. Let $U = {U_1, U_2,…,U_R}$ be the set of labels classes and $V={V_1, V_2,…,V_K}$ the set of cluster then,&lt;/p&gt;

\[\begin{align*}
\mathbf{1}_{U}(i,j) = 
\begin{cases}
1 &amp;amp; \text{if } x_i \in U_r \text{ and } x_j \in U_r , \quad 1 \leq r \leq R\\
0 &amp;amp; \text{if } \text{otherwise} \\ 
\end{cases} \\
\mathbf{1}_{V}(i,j) = 
\begin{cases}
1 &amp;amp; \text{if } x_i \in U_s \text{ and } x_s \in U_r ,\quad 1 \leq s \leq K\\
0 &amp;amp; \text{if } \text{otherwise} \\ 
\end{cases}
\end{align*}\]

&lt;h2 id=&quot;external-method-1--confusion-matrix&quot;&gt;External Method 1 : Confusion Matrix&lt;/h2&gt;
&lt;p&gt;which leads to the contingency table,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/continugency-table-expanded.png&quot; alt=&quot;contingency-table-expanded&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now each $n_{i,j}$ represents the number of times it is present. So say our class label is {Male, Female}, and from our clustering algorithm it classifies in 3 clusters ${V_1, V_2, V_3}$, then our contingency table would look like this&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/examples-contingency-table.png&quot; alt=&quot;example-contigency-table&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using this information in the contingency table we can create a confusion matrix,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/contingency-table.png&quot; alt=&quot;contingency-table&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where,&lt;/p&gt;

\[\begin{align*}
TP = \text{True positive} &amp;amp;= \text{model correctly predicts the positive class} \\ 
FP =  \text{False positive} &amp;amp;= \text{model incorrectly predicts the positive class} \\
TN = \text{true negative} &amp;amp;= \text{model correctly predicts the negative class} \\
FN = \text{false negative} &amp;amp;= \text{model incorrectly predicts the negative class}
\end{align*}\]

&lt;p&gt;How does this relate to our example? Given we have only two classes then for (class) Males are our positive class and (class) Female are our negative class. Then for the Males (positive) suppose  V_1 cluster the correct class for males and females are the negative class given by V_2,&lt;/p&gt;

\[\begin{align*}
TP &amp;amp;= \text{model correctly predicts the positive class} = 1   \\ 
FP &amp;amp;= \text{model incorrectly predicts the positive class} =  1 \\
TN &amp;amp;= \text{model correctly predicts the negative class} = 1\\
FN &amp;amp;= \text{model incorrectly predicts the negative class} = 0\\
\end{align*} \\\]

&lt;p&gt;Thus, our confusion matrix is,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/contingency-table-one-example.png&quot; alt=&quot;contingency-table-example&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;external-method-2-standard-measures-f-score-recall-precision-&quot;&gt;External Method 2: standard measures (F-score, Recall, precision )&lt;/h2&gt;
&lt;p&gt;Now to use standard measures we need to modify what standard measures met w.r.t clusters, generally precision, accuracy and F-score are defined as,&lt;/p&gt;

\[\begin{align*}
precision = \frac{TP}{TP + FP} \\ 
recall = \frac{TP}{TP + FN} \\ 
accuracy = \frac{TP + TN}{TP + TN + FN + FP}  \\ 
F1\text{-score} = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} \\ 
\end{align*}\]

&lt;p&gt;Given from all labels (males) how many did it correctly classify and recall is given Males labels in our algorithm did our algorithm get correct?
Now interms of clustering;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;precision: From our positive label what percentage did the algorithm correctly classify&lt;/li&gt;
  &lt;li&gt;recall: From our cluster how many what percentage of labels inside that cluster are correctly classified (positive)&lt;/li&gt;
  &lt;li&gt;accuracy : how of all cluster how many labels did the cluster get right all together&lt;/li&gt;
  &lt;li&gt;F1-score: Overall score performance of precision and accuracy&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;external-method-3-entropy-in-clustering-perspective&quot;&gt;External Method 3: Entropy in Clustering perspective&lt;/h2&gt;
&lt;p&gt;Entropy enables the abiilty to quantiy simialrities and dfifferences thoruhg probaiblity through measuring expected suprise. Generally entropy is measured as&lt;/p&gt;

&lt;p&gt;\(\begin{align*}
Entrophy &amp;amp;= \mathbb{E}(Surprise) \Longleftrightarrow\\
&amp;amp;= \sum{x P(X = x)} \Longleftrightarrow \\ 
&amp;amp;= \sum{log(\frac{1}{P(X = x)}) P(X = x)} \Longleftrightarrow\\
&amp;amp;= \sum{log(\frac{1}{p(x)}) p(x)} \Longleftrightarrow \\ 
&amp;amp;= \sum{p(x)(log(1) - log(p(x)))} \Longleftrightarrow \\ 
&amp;amp;= - \sum{p_i(x)log(p_i(x))}
\end{align*}\)
Now this is the entropy of a cluster i, and then the total entrophy is&lt;/p&gt;

\[\begin{align*}
Total\text{ }Entrophy &amp;amp;= \text{Sum of all Entropy Clusters}\Longleftrightarrow \\ 
e &amp;amp;= \sum_{i = 1}^k \frac{n_i}{n} e_i \\ 
\end{align*}\]

&lt;h1 id=&quot;internal-measures&quot;&gt;Internal Measures&lt;/h1&gt;
&lt;p&gt;Given we create a cluster without any labels, evaluating a cluster algorithm is extremely difficult. There are two main types,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cluster Cohesion - Measure compactness/tightness. How closely related are patterns in the same cluster?&lt;/li&gt;
  &lt;li&gt;Cluster separation - Measures isolation. How well separated are the clusters?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;internal-measures-1--cluster-cohesion&quot;&gt;Internal Measures 1 : Cluster Cohesion&lt;/h2&gt;
&lt;p&gt;Cluster Cohesion is a measure of spread out each cluster is, that is whats the dimension of a cluster’s&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/cluster-cohesion-example.png&quot; alt=&quot;cluster-cohesion-example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly, the width of left side cluster is larger then the right hand cluster, so left side cluster has lower cohesion score then right side cluster. Mathematicaly,
\(\begin{align*}
cohesion(C_i) = \sum_{x \in C_i , y \in C_i} proximity(x,y) \\ 
\end{align*}\)
proximity is any measure metric include Euclidean, Mahanttan, or L_p norm. Thus the smaller the proximity the samller then distance between each point within a cluster, thus the smaller the cohesion, and hence the better the result.&lt;/p&gt;

&lt;h2 id=&quot;internal-measures-2--cluster-separation&quot;&gt;Internal Measures 2 : Cluster separation&lt;/h2&gt;
&lt;p&gt;Cluster seperation measure inter-measure metric, which measure the distance between cluster, that is the futher away the cluster the higher the isolation, thus the hopefully better the cluster generalises.&lt;/p&gt;

&lt;h3 id=&quot;how-to-measure-cluster-separation&quot;&gt;How to measure Cluster separation&lt;/h3&gt;
&lt;p&gt;Similar, to creating clusters using aggolmetric hierarchical clustering, you can use a range of methods including single-method, complete-method, mean-method, centroid-method, all of which will create different interpretation of cluster separation metric. 
Mathematically cluster separation is defined as in single-method&lt;/p&gt;

\[\begin{align*}
d(C_i, C_j) = \text{min } d(x, y)
\end{align*}\]

&lt;p&gt;where $x_i$ are all elements in cluster $C_i$ and $y_i$ elements of cluster $C_j$.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I hope after reading this, both you and i have gained a more solid understanding of evaluating our clustering!&lt;/p&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="ML" /><summary type="html">This is a continuation of my previous blog post about “Hierarchial Clustering Algorithms”. In this blog i hope to teach you teach you a bit about how to evaluate you’re clustering algorithm.</summary></entry><entry><title type="html">Hierarchial Clustering: Intuition and Types</title><link href="http://localhost:4000/ml/hierarchial-clustering/" rel="alternate" type="text/html" title="Hierarchial Clustering: Intuition and Types" /><published>2024-05-24T00:00:00+10:00</published><updated>2024-05-24T00:00:00+10:00</updated><id>http://localhost:4000/ml/hierarchial-clustering</id><content type="html" xml:base="http://localhost:4000/ml/hierarchial-clustering/">&lt;p&gt;Today I faced a bit of a problem on my IBM UNSW data science problem, I have the feature “project_description” which includes a range of inputs ranging from “FACADE/ROOFS” to “FY16 RESO A IP SURVEILLANCE CAMERA INSTALLATION”. I recently used a automated labelling method for another feature, but this seem too big of a task for this feature. After researching for a bit I’v found another possible solution “Clustering”! So today I want to share a little overview about clustering, all information is from “Data and Knowledge Modeling and Analysis” via University of waterloo and University of North Caroline at Chapel Hill Chpt.7 Clustering techniques.&lt;/p&gt;

&lt;h1 id=&quot;what-is-clustering&quot;&gt;What is clustering?&lt;/h1&gt;

&lt;p&gt;It is form of unsupervised learning, which means there is nothing we a trying to predict or classify, rather we are trying to put samples together based on similarity, therefore our data is unlabeled. What does this really mean? In supervised learning our client our label feature “we are trying to predict project failure”, but in unlabeled data they still give use an objective, but there is not specific feature why are trying to predict or classify using.&lt;/p&gt;

&lt;p&gt;So in a sense we are trying to find inherent structure of the date, even though it may not entirely exist!&lt;/p&gt;

&lt;p&gt;Definition of clustering : group of instances based on similarity and dissimilarity.&lt;/p&gt;

&lt;p&gt;Regularly, clustering is used for market segmentation, that is given a set of data, form groups on with inherent characteristic (features) so that was can maximize return on advertisement.&lt;/p&gt;

&lt;h1 id=&quot;types-of-clustering-algorithms&quot;&gt;Types of clustering algorithms&lt;/h1&gt;

&lt;p&gt;There are four main types of clustering approaches:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Hierarchical Approach&lt;/li&gt;
  &lt;li&gt;Partitioning Approach&lt;/li&gt;
  &lt;li&gt;Density based approach&lt;/li&gt;
  &lt;li&gt;others - NN, Kernal, Affinity Prop&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Today I want to give intuition and overview of  hierarchical approaches to Clustering&lt;/p&gt;

&lt;h1 id=&quot;should-i-perform-clustering-on-cleaned-data-or-do-some-transformations&quot;&gt;Should I perform clustering on cleaned data or do some transformations?&lt;/h1&gt;

&lt;p&gt;Clustering can be performed directly on cleaned data, but also can be performed AFTER a dimention reduction algorithm is performed, like PCA. Majority of the times this usually leads to better reduced , since reducing dimentionality reduce non-importanly features and vairacne thus present a more accurate manifold/hyper-plane of the data.&lt;/p&gt;

&lt;h1 id=&quot;does-clustering-requres-a-training-pahse-like-supervised-learning&quot;&gt;Does Clustering requres a training pahse like supervised learning?&lt;/h1&gt;

&lt;p&gt;Since we are not trying to predict/classify our goals now is to organise data into groups thus we dont split our datase but rather trying to optimsie division of data with respect to an optimsiation criteria, we choose.&lt;/p&gt;

&lt;p&gt;When is clustering performed in a data sciecne pipeline.Permalink&lt;/p&gt;

&lt;p&gt;Clustering is usually perofrmed in the exploratory data anlytics stages&lt;/p&gt;

&lt;p&gt;Questions to ask when doing Clustering AlgorithmPermalink&lt;/p&gt;

&lt;p&gt;Some of the questions we ask priror to starting the clustering algorithm is;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How do we represent patterns? Do we represent them as sets of points?&lt;/li&gt;
  &lt;li&gt;How we we decided our measure of similarity? Remember in KNN algorithm we use distance in  norm to measure&lt;/li&gt;
  &lt;li&gt;How many cluster to we have? Again similar to KNN we decides the number of neightbour we choose, how do we know this is right?&lt;/li&gt;
  &lt;li&gt;How do we assess the performance of our clusters? Given we cluster a gorup, then how do we evaluate this?&lt;/li&gt;
  &lt;li&gt;How do we interpret the results?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;measuring-similarity-for-clustering&quot;&gt;Measuring Similarity for Clustering&lt;/h1&gt;

&lt;p&gt;Similar, to KNN our goals is to put points close to each other in a cluster while far apart points in different cluster, but consequently how would we determine 2 cluster if points are close by. In the examples below one ration method woulc be to cluster data points together (right corner), but how would we seperate the the points seperatly even thought they are as close to each other (middle points).&lt;/p&gt;

&lt;h1 id=&quot;hierarchical-algorithms&quot;&gt;Hierarchical Algorithms&lt;/h1&gt;

&lt;p&gt;This methods entails building a “hierarchy” of cluster. Generally, “hierarchy” means to order somethings from highest to lower or from lowest to highest. There are two types of&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Agglomerative - lowest to highest; start with a bunch of cluster then move to one cluster using a optimization criteria&lt;/li&gt;
  &lt;li&gt;Divisive - highest to lowest; start with 1 cluster then divide into multiple cluster using optimization criteria&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;visualising-the-problem&quot;&gt;Visualising the problem&lt;/h1&gt;
&lt;p&gt;Lets use an simple example, say we want given a client comes to use with customer information. The client ask’s ‘find customer segments for us to create a targeted a marketing strategy’. Now we are given only two features ‘Age’ of customer and ‘Annual Spending’. So the first step is to visualise the two groups.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/visualise-problem.png&quot; alt=&quot;visualise-cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see we only have 8 data-points in this case. Now we that maybe the best approach would be to cluster into maybe 4 clusters,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/human-intuition-clustering-example.png&quot; alt=&quot;human-intution-clustering&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How would a computer do this?&lt;/p&gt;

&lt;h1 id=&quot;agglomerative-hierarchical-algorithms&quot;&gt;Agglomerative Hierarchical Algorithms&lt;/h1&gt;

&lt;p&gt;Generally Agglomerative Hierarchical Algorithms are cateogrised in to these cateogries,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/aggol-cats.png&quot; alt=&quot;aggol-cateogries&quot; /&gt;&lt;/p&gt;

&lt;p&gt;for each graph method (Single/Complete/Grouped-Average) is present different methods of measuring simialirty, which leads to disicions of merging clusters. Which method you chose can chnages they way a cluster is merged.&lt;/p&gt;

&lt;h1 id=&quot;graph-methods--agglomerative-hierarchical-single-link-method&quot;&gt;Graph Methods : Agglomerative Hierarchical Single-Link Method&lt;/h1&gt;

&lt;p&gt;This is the most used method to perform hierarchical agglomerative clustering, in this case 
we measure similairty of cluster via min intra-cluster distance, that is, the minimum distance between two clusters, defined as&lt;/p&gt;

\[\begin{align*}
d(C_i, C_j) = \text{min } d(x_i, x_j)
\end{align*}\]

&lt;p&gt;where C represent cluster, \(x_i\) points in cluster \(C_i\) and \(x_j\) points in \(C_j\).&lt;/p&gt;

&lt;p&gt;So,  we start with finding the closest points in to a given to each point and then make each its own cluster, in this case it would be out human intuition,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/human-intuition-clustering-example.png&quot; alt=&quot;human-intution-clustering&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although in this case this may be the best number of cluster, in reality this may not be the case. Since all points are in a cluster we next try to find minimum distance to each neighbouring cluster. In this case the purple cluster is clostes to the botton on the green cluster is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/best-point-three-cluster-aggol.png&quot; alt=&quot;closest-pnt-cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and then join these clusters, so,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/three-cluster-algglomerative.png&quot; alt=&quot;three-clusters-agglomerative&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then we calculate each pnts distance inside the blue cluster to each pnt to other clusters. That is for say a given point in the purple cluster,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/finding-distances-three-cluster-agglomerative.png&quot; alt=&quot;distance-one-point-three-clusters-agglomerative&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So this is done for all points times this case in the purple cluster and then we find the mimum distance between a datapoint in purple to another cluster that is in this case,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/best-point-three-cluster-aggol.png&quot; alt=&quot;min-dist-point-three-clusters-agglomerative&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And so we combine these two clusters,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/two-clusters-aggol.png&quot; alt=&quot;two-clusters-agglomerative&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This continues until we finally have one cluster, that is all points are in the same cluster, which is where we initially started!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/one-cluster-aggol.png&quot; alt=&quot;one-clusters-agglomerative&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;graph-methods--agglomerative-hierarchical-complete-list-method&quot;&gt;Graph Methods : Agglomerative Hierarchical Complete-List Method&lt;/h1&gt;
&lt;p&gt;This takes a the simialr but opposie approach to single-list method, rather then combining cluster based on the minium between two points in between two cluster (intra-distacne), it uses the fursther points to form a cluster, defined by&lt;/p&gt;

\[\begin{align*}
d(C_i, C_j) = \text{max} d(p, p^&apos;)
\end{align*}\]

&lt;p&gt;Visually this looks like&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/complete-list-cluster-distance.png&quot; alt=&quot;complete-list-cluster-distance&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;geoemtric-agglomerative-hierarchical-methods&quot;&gt;Geoemtric Agglomerative Hierarchical Methods&lt;/h1&gt;

&lt;p&gt;From the geometric part of Agglomerative Hierarchical Methods, each method is extremely intuitive.
For Agglomerative Hierarchical Clustering Centriod  Methods, we find the centriod (centre) of the cluster and then we measure simularity simialr to single-method based on \(min d(c_i, c_j)\) so we find the clostest two centriod and merge them together. This continue untill all data points are in one cluster!&lt;/p&gt;

&lt;h1 id=&quot;how-to-effectivley-visualise-this-our-different-options-for-clusters&quot;&gt;How to effectivley visualise this our different options for clusters?&lt;/h1&gt;

&lt;p&gt;In most cases we dealing with multidimentional features space, so visualising it impossible unless we do some dimension reductionality technique like PCA etc. So the most effective way to visualise thses different potential cluster is through tree-diagrams, althought there are other methods including banner, point representation, etc. There are two main types of tree diagrams used in clustering representation dendrograms and n-tree.&lt;/p&gt;

&lt;h2 id=&quot;dendrograms&quot;&gt;Dendrograms&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/dendogram-aggol.png&quot; alt=&quot;dendogram-aggol-clustering&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So in the for a agglomerative hierarchical algorithms we start at the bottom and compute, even thought we compute the distance between each point and all other points, we only graph the closest point in the dendrograms. In this case, point 3 was clostes to point 8 with distance of 0.07, while for point 4 closest point was point 6.&lt;/p&gt;

&lt;p&gt;Importantly, for point 1 the clostes point was point 4 (not shown in graph), so bacuase of this it form 1 big cluster with point 1, 4, 6 rather then point 4 only since point 4 clostes point isn’t 1 but 6.&lt;/p&gt;

&lt;h2 id=&quot;n-tree&quot;&gt;n-tree&lt;/h2&gt;
&lt;p&gt;Below, internal nodes are clusters (A,B,C) whilst the terminal notes/leaves are the data points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/n-tree-diagram.png&quot; alt=&quot;n-tree-diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Importantly a dendrograms is a type of n-tree, which all internal notes satisify,&lt;/p&gt;

\[\begin{align*}
h(A) \leq h(C) \Longleftrightarrow A \subseteq C
\end{align*}\]

&lt;p&gt;where h: height on n-tree. And A and C a clusters. Intutivley, clearly on our 5-tree diagram h(A) less then h(C) this makes sense we reber back to the n-tree diagram clearly if were were to draw clusters then C takes all points  to , but Cluster A only contains  to  a subset of elements and C. So&lt;/p&gt;

&lt;h1 id=&quot;divisive-hierarchical-clustering&quot;&gt;Divisive Hierarchical Clustering&lt;/h1&gt;

&lt;p&gt;In this instance we want to do the opposite of of agglomerative clustering where we start we one large cluster and then break into the smallest cluster (ak.k a cluster containing only two points). There are two main types of Divisive Hierarchical Clustering;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;monothetic - we use one feature as a basis of cluster division&lt;/li&gt;
  &lt;li&gt;polythetic - take all features as basis of cluster division&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Lets show an example of single-link Divisive Hierarchical Clustering, where we measure simialirty by minium distance. In this case we sperate points which are further away since they show the highest dismilarity in this method. So given,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/divisive-min-spanning-one-cluster.png&quot; alt=&quot;divisive-mini-span-one-cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;the lies show the Minimum Spanning tree. Clearly, the point with the shortest distance to all other points is P2 to P5, therefore we break that ed and create two clusters as so,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/clustering/divise-two-cluster.png&quot; alt=&quot;divise-two-cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This continue until each point has it own cluster. Now importantly we can also use single-methods complete method, mean-method or any methods, but this chnages our similarity metric&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;And there we have it! I hope you have gained some intuition about Heiarchial Clustering !&lt;/p&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="ML" /><summary type="html">Today I faced a bit of a problem on my IBM UNSW data science problem, I have the feature “project_description” which includes a range of inputs ranging from “FACADE/ROOFS” to “FY16 RESO A IP SURVEILLANCE CAMERA INSTALLATION”. I recently used a automated labelling method for another feature, but this seem too big of a task for this feature. After researching for a bit I’v found another possible solution “Clustering”! So today I want to share a little overview about clustering, all information is from “Data and Knowledge Modeling and Analysis” via University of waterloo and University of North Caroline at Chapel Hill Chpt.7 Clustering techniques.</summary></entry><entry><title type="html">Bayesian statistics vs frequentist statistics : Estimating Probability From Data</title><link href="http://localhost:4000/ml/Estimate-Probability-from-data/" rel="alternate" type="text/html" title="Bayesian statistics vs frequentist statistics : Estimating Probability From Data" /><published>2024-05-21T00:00:00+10:00</published><updated>2024-05-21T00:00:00+10:00</updated><id>http://localhost:4000/ml/Estimate-Probability-from-data</id><content type="html" xml:base="http://localhost:4000/ml/Estimate-Probability-from-data/">&lt;p&gt;Today I was watching Kilian Weinberger lecture on Estimating Probabilities from Data: Maximum Likelihood Estimation” Cornell CS4780 SP17 and now I finally understand the difference between the types of statistics (Also I read “Probabilistic Machine Learning: An Introduction” by Kevin Murphy). The following post will discuss the difference between the two most common schools of statistics, through the lens of machine learning, data estimation using probaiblity; MLE and MAP.&lt;/p&gt;

&lt;h1 id=&quot;recall&quot;&gt;Recall&lt;/h1&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;An important part of why machine can learning is that we assume that all samples points taken are from the same distribution, $ (x_i, y_i) ~ P, \forall i \in \mathbb{Z}^{+} $ and are i.i.d. Now using this assumption which is the backbone of all learning algorithms, then there $ \exists P(x,y) $, for which a conditional expectation can also be calculated, $ P(y&lt;/td&gt;
      &lt;td&gt;x) $, that is, given a feature what the the probability that it lands on this specific label. Alternatively , $ P(x&lt;/td&gt;
      &lt;td&gt;y) $ must also exist, that is given a label what is the probability that it displays certain features. The study of both conditional probabilities are separated using Bayes Theorem,&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[\begin{align*}
    P(x | y) &amp;amp;\propto P(y | x)P(x) \quad\quad\text{Discriminative Learning} \\ 
    P(y | x) &amp;amp;\propto P(x | y)P(y) \quad\quad\text{Generative Learning}
\end{align*}\]

&lt;p&gt;In essence, discriminative learning aims to create explicit boundaries between classes of labels, similar to SVMs, Perceptron, and k-NN. On the other hand, generative learning tries to model the distribution of individual classes, such as Naive Bayes or Gaussian Mixture Models (GMM). Within each of these types of learning, you can have parametric models, where we assume the distribution of ( p(x,y) ) comes from a certain well-known distribution like normal, Bernoulli, exponential, or non-parametric.&lt;/p&gt;

&lt;h1 id=&quot;what-is-a-parameter&quot;&gt;What is a parameter?&lt;/h1&gt;
&lt;p&gt;given some distribution a parameter is a point estimate that characteristics a known class of distribution. I’m sure you have heard of common well know distribution including $N(\mu, \sigma^2)$ or $ \mathrm{Bern}(\lambda)$. In this case $\theta$, our parameter can be a set, or singular point estimate which character our distribution so $\theta = (\sigma^2, \mu)$ or $\theta = (\lambda) $&lt;/p&gt;

&lt;h1 id=&quot;motivating-example&quot;&gt;Motivating example&lt;/h1&gt;

&lt;p&gt;For a motivating exmaples consdier flipped a equally wieghted coin. Say we coin and we get \(D = \{ T , T, T, H, H, H, H, T, T, T, T \}\). Now considering it is eually wieghted coin we would hope that our probbilities of getting each side of the dice as \(\frac{1}{2}\) something from pervious years we have come to know. Say we want to know what is the probaiblites of getting a head, then the most intuitive method would be to&lt;/p&gt;

\[\begin{equation*}
    P(\text{getting heads}) = \frac{n_h}{n_t + n_h}
\end{equation*}\]

&lt;p&gt;where $ \text{n} = \text{Number of times gotten a side}$ . Yet it used this equation for our set of outcomes $ D $, we find $ P(\text{heads}) = \frac{4}{11} $, which isn’t 50 \% as we expected. Why is that did we not find the right probability?&lt;/p&gt;

&lt;h1 id=&quot;frequentist-vs-bayesian-n-perspective&quot;&gt;Frequentist vs Bayesian n Perspective&lt;/h1&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;When studying probability theory, we are concerned with $ p(D&lt;/td&gt;
      &lt;td&gt;\theta) $, which models the distribution with some known $ \theta $. On the other hand, in statistics, previously known as inverse probability theory, we are given some data, and we aim to infer the unknown parameters $ \theta $ given observations, $ p(\theta&lt;/td&gt;
      &lt;td&gt;D) $.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Generally, Frequentist are concerned with ‘frequency’, meaning that over time, the asymptotic behavior of our probability will converge to a certain number, as in the example above. Importantly, this requires a lot of data; otherwise, we would be overfitting. On the other hand, Bayesian statisticians know that data is readily available, and indeed, we have some assumptions we know about the data. Thus, they assume some ‘prior’ distribution of $p(\theta)$ from expertise, enabling them to readily use it when a small sample size is available.&lt;/p&gt;

&lt;h1 id=&quot;frequentist-mle&quot;&gt;Frequentist: MLE&lt;/h1&gt;

&lt;p&gt;So, we define&lt;/p&gt;

\[\begin{align*}
p(\mathcal{D}|\theta) = \prod_{n=1}^N p(y_n|x_n, \theta)
\end{align*}\]

&lt;p&gt;that is our i.i.d assumption of machine learning. Then applying a log (we use to simply computation), we get the log-likelihood,&lt;/p&gt;

\[\begin{align*}
\mathcal{L}(\theta) = \sum_{n=1}^N log( p(y_n|x_n, \theta)).
\end{align*}\]

&lt;p&gt;So, then we define MLE of a parameter&lt;/p&gt;

\[\hat{\theta}_{MLE} = \arg\max_{\theta} \sum_{n=1}^N \log p(y_n | x_n, \theta)\]

&lt;p&gt;So optimisations theory is easier, to optimise a function through minimize cost functions so,&lt;/p&gt;

\[\theta_{\text{MLE}}^{\hat{}} = \arg\max_{\theta} - \sum_{n=1}^N \log p(y_n | x_n, \theta)\]

&lt;h1 id=&quot;mle-example&quot;&gt;MLE example&lt;/h1&gt;
&lt;p&gt;Going back to our examples we flips coins, we now can assume that $p(x,y) ~ Bern(\lambda) $, where now $\theta = P(Y = 1) $, where $Y = number of times of getting heads heads$&lt;/p&gt;

&lt;p&gt;So,&lt;/p&gt;

\[\begin{align*}
    \mathcal{L}(\theta) = \arg\max_{\theta} - \left( \sum_{n=1}^N \mathbb{1}_{y_n=1} \log(\theta) + \mathbb{1}_{y_n=0} \log(1 - \theta) \right).
\end{align*}\]

&lt;p&gt;Let, $ N1 = \sum_{n=1}^N \mathbb{1}&lt;em&gt;{y_n=1}$ and $ N0 =  sum&lt;/em&gt;{n=1}^N \mathbb{1}&lt;em&gt;{y_n=0} $. Now, we can use differentiating and let the loss function equal 0 to find the minimum (like in high school finding the minimum point of a function) to find $\hat{\theta}&lt;/em&gt;{MLE}$,&lt;/p&gt;

\[\begin{align*}
    \hat{\theta}_{MLE}= \frac{N1}{N0 + N1}
\end{align*}\]

&lt;p&gt;Isn’t this similar to our intuition right? This is known as classical statistics, or Frequentist statistics. Importantly we done assume  $\theta$ has any distribution itself but rather is a parameter which contains information about the distribution of a data points.&lt;/p&gt;

&lt;h1 id=&quot;what-is-we-didnt-get-any-heads-in-out-sample&quot;&gt;What is we didnt get any heads in out sample?&lt;/h1&gt;
&lt;p&gt;he $ \hat{\theta}_{MLE} = 0 $. Is this correct? Well, of course not. We know from our intuition it should be $ \frac{1}{2} $. So Frequentists use this method, knowing we (plus one) Laplace Smoothing to ensure in the case of our sample not having an event occurring,&lt;/p&gt;

\[\begin{equation*}
\hat{\theta}_{MLE}= \frac{N1 + 1 }{N0 + N1 + 2}
\end{equation*}\]

&lt;p&gt;Since there is only two classes in our examples head or tails so we add 2 in the denominator.&lt;/p&gt;

&lt;h1 id=&quot;map&quot;&gt;MAP&lt;/h1&gt;
&lt;p&gt;Alternatively in Bayesian statistics we don’t consider Laplace smoothing or $\theta$ as just a parameter but rather in build upon the notion that we don’t have allot of data and data can’t always model the distribution of the sample accurately. So we make this assumption (adding bias) that $ \theta $ random variable. Now a random variable is neither a variables or random but rather a function which maps points of a measurable set $ \sigma$-algebra to the real number line, but a important property is that random variables can have distributions. So, now we can express to new equations, using Bayes Theorem,&lt;/p&gt;

\[\begin{align*}
P(\theta | D) &amp;amp;= \frac{P(D | \theta) P(\theta)}{ P(D)} \Rightarrow \\ 
P(\theta | D) &amp;amp;\quad\propto\quad P(D | \theta) P(\theta). \\ 
\end{align*}\]

&lt;p&gt;Now in Bayesian Statistics, each component is named,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$ P(D&lt;/td&gt;
          &lt;td&gt;\theta) $ Likelihood of data given ( \theta )&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$ P(\theta&lt;/td&gt;
          &lt;td&gt;D)$ (posterior distribution): Distribution over the parameter(s) ( \theta ) after we have observed the data&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;$P(\theta) $ (prior): Distribution over the parameter(s) ( \theta ) before we see any data. 
We need to choose the prior distribution using our expertise and knowledge which of course can work out great, or horribly wrong if we assume the distribution of our prior wrong. So now,&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
     p(\mathcal{D}|\theta) &amp;amp;= \prod_{n=1}^N p(y_n|x_n, \theta)p(\theta) \\
\end{align*}\]

&lt;p&gt;Applying log,&lt;/p&gt;

\[\begin{align*}
    \mathcal{L}(\theta)&amp;amp;= \sum_{n=1}^N log(p(y_n|x_n, \theta))+ \sum_{n=1}^N log( p(\theta)) \\
\end{align*}\]

&lt;p&gt;So,&lt;/p&gt;

\[\begin{align*}
    \hat{\theta}_{map} = \arg\max_{\theta} \mathcal{L}(\theta)
\end{align*}\]

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Going back to the coin flip we can assume our prior distribution (guess) is $  p(\theta) = Beta(\theta&lt;/td&gt;
      &lt;td&gt;a, b) $ then our log likelihood ends up becoming&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

\[\begin{align*}
    \mathcal{L}(\theta) = &amp;amp;\left( \sum_{n=1}^N \mathbb{1}_{y_n=1} \log(\theta) + \mathbb{1}_{y_n=0} \log(1 - \theta) \right) + [(a - 1)log(\theta) + (b - 1)log(1 - \theta)]
\end{align*}\]

&lt;p&gt;Then using normal minimisation method of finding first derivative letting it equal to zero to find minimizer for $\hat{\theta})$ we get&lt;/p&gt;

\[\begin{align*}
    \hat{\theta}_{map} = \frac{N1 + a - 1}{N1 + N0 + a + b - 2}
\end{align*}\]

&lt;p&gt;So by assuming a prior distribution in Bayesian statistics we end up getting a very similar estimator found in Frequentist statistics using Laplace Smoothing. So in a sense they are very similar!&lt;/p&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="ML" /><summary type="html">Today I was watching Kilian Weinberger lecture on Estimating Probabilities from Data: Maximum Likelihood Estimation” Cornell CS4780 SP17 and now I finally understand the difference between the types of statistics (Also I read “Probabilistic Machine Learning: An Introduction” by Kevin Murphy). The following post will discuss the difference between the two most common schools of statistics, through the lens of machine learning, data estimation using probaiblity; MLE and MAP.</summary></entry><entry><title type="html">How sampling distribution is the link between Machine Learning and data science</title><link href="http://localhost:4000/random/distribution-link-datascience-ml/" rel="alternate" type="text/html" title="How sampling distribution is the link between Machine Learning and data science" /><published>2024-05-18T00:00:00+10:00</published><updated>2024-05-18T00:00:00+10:00</updated><id>http://localhost:4000/random/distribution-link-datascience-ml</id><content type="html" xml:base="http://localhost:4000/random/distribution-link-datascience-ml/">&lt;h1 id=&quot;ever-wondered-how-these-two-field-like-with-concrete-examples&quot;&gt;Ever wondered how these two field like with concrete examples?&lt;/h1&gt;
&lt;p&gt;Data science formull is defined “statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data”. In a informal sense we are trying to transform raw data into strucutre data for a specific task. Now this take can range from mere simple data analysis to more complex task including classifying data or regression problems. Now, one role of data sciecne is to provide data to machine learning engineers for a spcifci learning problem.  So the next question begins ‘what does the macvhien learnign requires from a data scientist to ensrue data is correct?’. We will look at the basics of machine learning algorithsm, including loss functions, assumptions and then devlve into the importance of data quality given by data scientist.&lt;/p&gt;

&lt;h1 id=&quot;little-about-machine-learning&quot;&gt;Little about machine learning&lt;/h1&gt;
&lt;p&gt;Machine learning is a process where by a machine, usually a program, learns from given data where by we have input, \(X\) and output \(Y\), known as feature inputs and labels, respectively. Now the goals is to create a program which can adequatly be able to given predict ‘label’ given a new input. An exmaples can be classify females and males, know a data scientist will go out and take picture of males and females, and then label them both males or female, then this is given to a machine leanring program for which is looks at data and tries to understand distinction bewteen male and females, untill it get very good at it. Then we will given it new picture and then hopefully it will be able to identify correctly geneder of the male or female.&lt;/p&gt;

&lt;h1 id=&quot;now-an-important-assumptio-we-make-is-machine-learning-is-that-all-samples-take-have-the-same-distribtion-p-and-are-iid-why&quot;&gt;Now an important assumptio we make is Machine learning is that all samples take have the same distribtion \(P\) and are i.i.d why?&lt;/h1&gt;

&lt;p&gt;This creates the link between machien learning and data sciecne. Ifall possible data that we can be given is from the same distribition, since if they it would be impossible to predict or classify random selected people since we a any random input.&lt;/p&gt;

&lt;h2 id=&quot;example--gender-shades-project-2018&quot;&gt;Example : Gender Shades project 2018&lt;/h2&gt;

&lt;p&gt;In 2018 gender classification algorithms developed by IBM and Microsoft was studye based on miclassification based upon skin tone, from four cateogires: darker-skinned females, darker-skinned males, lighter-skinned females, and lighter-skinned males. All three algorithms performed the worst on darker-skinned females, with error rates up to 34% higher than for lighter-skinned males.&lt;/p&gt;

&lt;p&gt;A key reaosn might be the samples distribution, now say the task to recognise face, if a data scientist provides face of only light-skinned males then cleary the algorithsm will only be able to identify these indviduals and fall short on other skins tones. So the distribution we wrong for the task since the data that the algorith was trained was different to the actually dsitirbution it was testet on&lt;/p&gt;

&lt;h1 id=&quot;continying&quot;&gt;Continying&lt;/h1&gt;
&lt;p&gt;Additionaly, these samples must be i.i.d, so we have already adressed the identifcallty distribution, but indepecne is importance is to ensure that these samples done effect one another cause if they do it would be impossible to find the relationship between two samples (x_i, y_i)&lt;/p&gt;

&lt;h1 id=&quot;how-a-machine-algorithm-works-using-out-asusmption&quot;&gt;How a Machine algorithm works using out asusmption&lt;/h1&gt;

&lt;p&gt;So currently I’ve talked about the assumption that all possible data that an algorithsm will very see is given by data sciecntist so the roels of trhe data sciecne to provides data which will represent all possible data it will be given in the future.&lt;/p&gt;

&lt;h1 id=&quot;how-then-does-a-machine-learning-engineer-optimise-this-data&quot;&gt;How then does a Machine learning engineer optimise this data?&lt;/h1&gt;
&lt;p&gt;(add picture)
so out goals to minimise the error rates of our algorithm chosen h, we do this through choose a loss function which is appropriate to the task, now importantly iv talked about teh improtance of the algorithm bening able to generalise to any data set and that the data, of course  is assumpted to from the same distribution \(P\). Our easier first guess is for out loos function&lt;/p&gt;

&lt;p&gt;l(h;D)&lt;/p&gt;

&lt;p&gt;but the key problem is that this only optimsised out dataset in a sense it memories out data givne but give new sample it would not be able to rpedict it well, so we must create a denfition which generlaise well for all possible data from a asusmed distirbution 
\(E(l(X,Y))_{(x_i,y_i) ~ P}\)
That is we want to find the middle weight of all possible samples from data future and past&lt;/p&gt;

&lt;h1 id=&quot;how-we-we-implement-this-into-our-machine-leanring-program&quot;&gt;How we we implement this into our machine leanring program?&lt;/h1&gt;

&lt;p&gt;This is why we split data into test and training set, for the sole reason is that out assumption is all points are (xi,yi) ~ P so we act like our data is a represnetaiotn of the whole population of task we need to learn and for that reason we only touch our test set for final evaluation process. we it would givne use a interpretation of how well&lt;/p&gt;

&lt;h1 id=&quot;conlusion&quot;&gt;Conlusion&lt;/h1&gt;
&lt;p&gt;So the reaosn why data sciecne is importacne of machine learning engineers is that since machine leanirng engineeers assume that all data for a learnign as has the same distribution, the data scienctist roles is to ensure that the data they provides represent all possible inputs for that task.&lt;/p&gt;</content><author><name>Shahid Hussain</name><email>shahid.hussain0120@gmail.com</email></author><category term="Random" /><summary type="html">Ever wondered how these two field like with concrete examples? Data science formull is defined “statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data”. In a informal sense we are trying to transform raw data into strucutre data for a specific task. Now this take can range from mere simple data analysis to more complex task including classifying data or regression problems. Now, one role of data sciecne is to provide data to machine learning engineers for a spcifci learning problem. So the next question begins ‘what does the macvhien learnign requires from a data scientist to ensrue data is correct?’. We will look at the basics of machine learning algorithsm, including loss functions, assumptions and then devlve into the importance of data quality given by data scientist.</summary></entry></feed>