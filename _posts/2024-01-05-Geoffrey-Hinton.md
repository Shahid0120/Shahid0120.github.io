---
layout: single
title: "Geoffrey Hinton : The godfather Deep learning?"
header:
categories:
  - Inspiration
author_profile: false
---
<!---
Interview 
- High School friedn -> mathematics bettwe then him states -> the brain uses holigram -> Whats a holigram? "Lashes experiement with mice where u chop bits of rats brains and where bits of brains are distrib uted over the whole brain" 
- university -> physiology + physics -> became a carpenter -> when to cmabridge to study AI -> where his professor just finsihed working with NN 
- PHD in AI -> couldnt get jobs in britian -> when to America (California)
- Ron Williams, between us developed the backprop algorithm, it was mainly David Rumelhart's idea. Paul Werbos had published it already quite a few years earlier, but nobody paid it much attention. And there were other people who'd developed very similar algorithms, it's not clear what's meant by backprop. But using the chain rule to get derivatives was not a novel idea. -> Back propigation 
- which one is the most exited? So I think the most beautiful one is the work I do with Terry Sejnowski on Boltzmann machines. So we discovered there was this really, really simple learning algorithm that applied to great big density connected nets where you could only see a few of the nodes. So it would learn hidden representations and it was a very simple algorithm. And it looked like the kind of thing you should be able to get in a brain because each synapse only needed to know about the behavior of the two neurons it was directly connected to.
- Finish wathcing it
--!>
